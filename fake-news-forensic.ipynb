{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":2345298,"sourceType":"datasetVersion","datasetId":1415853}],"dockerImageVersionId":30918,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# **Fake News Forensic ‚Äì Detecting & Summarizing Misinformation with GenAI**\n\nThis notebook demonstrates how to detect and summarize misinformation (so-called ‚Äúfake news‚Äù) by combining:\n\n1. **Embeddings** to transform text into vector representations.\n2. A **Vector Store** (in this example, using FAISS) to enable similarity search.\n3. **Retrieval-Augmented Generation (RAG)** techniques to ground outputs from a Large Language Model (LLM) in factual data.\n\nWe will walk through each step‚Äîfrom data loading and vector database creation to prompting a language model for fact-checking responses. Feel free to **experiment** by changing prompts, using different LLMs, or customizing the data!\n\n","metadata":{}},{"cell_type":"markdown","source":"# **1. Introduction**\n\nMisinformation spreads rapidly online, and manual fact-checking cannot always keep pace. In this project, we show how **Generative AI** methods can help:\n\n1. **Find** relevant fact-check statements from a curated dataset (using embeddings + a vector database).\n2. **Generate** short, evidence-based explanations (using a language model like OpenAI‚Äôs GPT).\n3. **Structure** responses in a user-friendly format, such as JSON or bullet-point summaries.\n\nOur example uses the **ISOT Fake News Dataset** (which labels articles as true or false) and an OpenAI model to produce short explanations referencing the retrieved evidence.\n","metadata":{}},{"cell_type":"markdown","source":"# **2. Libraries & Setup**\n\nWe first need to install and import the libraries that will power our pipeline:\n\n- **sentence-transformers**: For creating text embeddings.\n- **langchain** & **chromadb**: Common libraries for building LLM applications (though we focus on FAISS here).\n- **faiss-gpu**: A vector store for similarity search.\n- **pandas**, **numpy**: For data manipulation.\n- **torch**: Underlying framework (used by sentence-transformers).\n- **openai**: To interact with OpenAI‚Äôs GPT models.\n\nInstalling might require a restart of the environment once done. Let‚Äôs go ahead and set things up.\n","metadata":{}},{"cell_type":"code","source":"!pip install -q sentence-transformers langchain chromadb faiss-gpu","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n\nimport pandas as pd\nimport numpy as np\nimport torch\n\n# For embeddings (SentenceTransformers)\nfrom sentence_transformers import SentenceTransformer\n\n# For vector database, example: FAISS or Chroma\nimport faiss  # or use Chroma or another\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"We import the necessary libraries for data manipulation (**pandas**, **numpy**), generating embeddings (**SentenceTransformers**), creating a vector index (**FAISS**), and working with **PyTorch** (which powers the transformer model).\n","metadata":{}},{"cell_type":"markdown","source":"**Obtain an API Key**\n\nTo use OpenAI‚Äôs API (e.g., GPT-3.5 or GPT-4), you will need an API key from [https://platform.openai.com/](https://platform.openai.com/). Note that **paid plans** or billing information might be required depending on how many requests you make and the rate limits you exceed. If you are just testing small requests, the free trial credit might suffice, but for consistent or larger-scale usage, you‚Äôll need a paid subscription.\n\nIn a Kaggle notebook or a local Jupyter environment, you can store the key in an environment variable or a secrets manager. Below, we demonstrate retrieving it from Kaggle‚Äôs `UserSecretsClient`.\n","metadata":{}},{"cell_type":"code","source":"# For language model calls (OpenAI, Hugging Face, or local)\n# Example with OpenAI:\n\nimport os\nimport openai\nfrom kaggle_secrets import UserSecretsClient\n\nOPENAI_API_KEY = UserSecretsClient().get_secret(\"OPENAI_API_KEY\")\nopenai.api_key = OPENAI_API_KEY\n\n\n\n# ...any additional imports...\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# **3. Data Loading & Preprocessing**\n\nIn this section, we‚Äôll load our **ISOT Fake News Dataset**, which consists of two CSV files:\n- `Fake.csv` for misinformation articles\n- `True.csv` for real news articles\n\nWe then combine them into a single DataFrame, adding a column `'label'` indicating whether the text is **true** or **false**. You can swap in **your own** data here to create a customized fact-checking pipeline.\n","metadata":{}},{"cell_type":"code","source":"# 1) Read the CSVs from the ISOT Fake News Dataset\ndf_fake = pd.read_csv('/kaggle/input/isot-fake-news-dataset/Fake.csv')\ndf_true = pd.read_csv('/kaggle/input/isot-fake-news-dataset/True.csv')\n\n# 2) Assign labels\ndf_fake['label'] = 'false'\ndf_true['label'] = 'true'\n\n# 3) Concatenate into a single DataFrame\ndf = pd.concat([df_fake, df_true], ignore_index=True)\n\n# 4) Display 3 rows from each label to get a quick look\ndf_subset = df.groupby('label').head(3)\ndf_subset\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"You can see a snippet of the data above. Each row contains text (the article body or headline) and the `label` indicating **true** or **false**.\n","metadata":{}},{"cell_type":"markdown","source":"# **4. Building & Populating the Vector Store**\n\nIn this stage, we will:\n1. **Generate embeddings** for each article/claim using a Sentence Transformers model.\n2. **Store** those embeddings in a FAISS index.\n\nA vector store allows us to quickly find articles similar to any new query. This is critical for retrieval-augmented generation: we can feed the top-matching articles to the language model to ground its responses in factual data.\n","metadata":{}},{"cell_type":"code","source":"# Choose a sentence-transformers model (lightweight example)\nembedding_model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2', device='cuda')\n\n# Create a list of texts to embed (e.g., headlines or claims)\ntexts = df['text'].tolist()\nlabels = df['label'].tolist()\n\n# Generate embeddings\nembeddings = embedding_model.encode(texts, show_progress_bar=True)\n\nprint(\"Embeddings shape:\", embeddings.shape)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Here, we use the **all-MiniLM-L6-v2** model, which produces 384-dimensional vector embeddings. If you prefer a different model (e.g., `all-mpnet-base-v2` or a multilingual model), simply replace the string in `SentenceTransformer(...)`.\n","metadata":{}},{"cell_type":"markdown","source":"**4.2 Creating a FAISS Index**\n\nNext, we create a **FAISS** index to store these embeddings. FAISS supports efficient similarity search, letting us retrieve the top-k most similar items for any new query.\n\nYou could use other vector databases (e.g., **Chroma**, **Milvus**, **Pinecone**, etc.) if you prefer.\n","metadata":{}},{"cell_type":"code","source":"dimension = embeddings.shape[1]\nindex = faiss.IndexFlatL2(dimension)\nindex.add(embeddings)\n\nprint(f\"FAISS index size: {index.ntotal}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"The index is now created and populated. We can quickly query it with any embedding vector to find similar texts from our dataset.\n","metadata":{}},{"cell_type":"markdown","source":"# **5. Retrieval Augmented Generation (RAG)**\n\nWith the vector store in place, we can **retrieve** the top relevant articles for a new claim/headline. Then, we‚Äôll feed both the user‚Äôs query and the retrieved evidence into an LLM to generate a short, fact-based explanation.\n\nThis approach helps **reduce hallucinations** by giving the model actual reference text for each claim. Let‚Äôs define some helper functions next.\n","metadata":{}},{"cell_type":"markdown","source":"**5.1 Example: Single Query with RAG**\n\nBelow, we:\n1. Create a function to **retrieve** the top-k similar entries using the FAISS index.\n2. Create a function to **generate** a short explanation by referencing those retrieved entries in the LLM prompt.\n3. Test the workflow with an example query (e.g., `\"Vaccines cause autism.\"`).\n\nYou can swap in **any query** you‚Äôd like here. Also, if you want to use GPT-4 or a different model, update the `model` parameter in the `openai.chat.completions.create(...)` call.\n","metadata":{}},{"cell_type":"code","source":"\nimport openai\n\ndef retrieve_similar_texts(query, k=3):\n    \"\"\"\n    Given a query (string), return top k similar text entries\n    from the FAISS index.\n    \"\"\"\n    query_emb = embedding_model.encode([query])\n    distances, indices = index.search(query_emb, k)\n\n    results = []\n    for idx in indices[0]:\n        item_text = df.loc[idx, 'text']\n        item_label = df.loc[idx, 'label']\n        results.append((item_text, item_label))\n    return results\n\ndef generate_explanation(query, retrievals):\n    \"\"\"\n    Use the retrieved text & label to produce a short explanation\n    via ChatCompletion in openai>=1.0.0.\n    \"\"\"\n\n    references_str = \"\\n\".join([f\"Claim: {rt[0]} -- Label: {rt[1]}\" for rt in retrievals])\n\n    messages = [\n        {\n            \"role\": \"system\",\n            \"content\": \"You are a fact-checking assistant.\"\n        },\n        {\n            \"role\": \"user\",\n            \"content\": f\"\"\"\nThe user says: '{query}'\n\nWe found these fact-check references:\n{references_str}\n\nBased on these, is the user's claim likely true or false?\nProvide a concise explanation (2-3 sentences) referencing the evidence above.\n            \"\"\"\n        }\n    ]\n\n    # For openai>=1.0.0, we use the 'chat.completions.create' endpoint\n    response = openai.chat.completions.create(\n        model=\"gpt-3.5-turbo\",  # or gpt-4, etc.\n        messages=messages,\n        max_tokens=150,\n        temperature=0\n    )\n\n    # Access the content by attribute, not by subscripting\n    return response.choices[0].message.content.strip()\n\n# Test call\nquery_test = \"Vaccines cause autism.\"\nretrieved = retrieve_similar_texts(query_test, k=3)\nllm_explanation = generate_explanation(query_test, retrieved)\n\nprint(\"----Retrieved Fact Checks----\")\nfor i, (txt, lbl) in enumerate(retrieved):\n    print(f\"{i+1}) [Label: {lbl}] {txt[:80]}...\")\n\nprint(\"\\n----LLM Explanation----\")\nprint(llm_explanation)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"The prompt to the LLM includes both the user‚Äôs claim and the top matching fact-check entries. Feel free to **tweak** the instructions to get a more structured output, longer explanation, or bullet-point summary. This is a key part of **prompt engineering**.\n","metadata":{}},{"cell_type":"markdown","source":"# **6. Demonstration of Additional Capabilities**\n\nBelow is an example of **few-shot prompting** to ensure that the LLM outputs data in a consistent format (e.g., JSON). This can be helpful if you want to parse the output programmatically later on.\n","metadata":{}},{"cell_type":"markdown","source":"**Few-Shot Prompting Example**\n\nWe might want the explanation in a more structured JSON format or a ‚Äúbullet-point‚Äù style. Let‚Äôs do a few-shot approach to ensure consistent formatting.","metadata":{}},{"cell_type":"code","source":"import openai\n\ndemo_prompt = \"\"\"\nBelow are examples of how to respond to fact-check queries in JSON:\n\nExample 1:\n{\n  \"label\": \"false\",\n  \"explanation\": \"This claim is contradicted by evidence in X and Y...\"\n}\n\nNow, follow that format exactly:\n\nUser Claim: \"5G towers spread COVID-19.\"\nEvidence: \"Claim: 5G networks cause coronavirus. Label: false\"\n\n{\n   \"label\": \n   \"explanation\":\n}\n\"\"\"\n\n# In openai>=1.0.0, you call openai.chat.completions.create(...)\nresponse_json = openai.chat.completions.create(\n    model=\"gpt-3.5-turbo\", \n    messages=[\n        {\n            \"role\": \"system\",\n            \"content\": \"You are a helpful assistant that outputs JSON.\"\n        },\n        {\n            \"role\": \"user\",\n            \"content\": demo_prompt\n        }\n    ],\n    max_tokens=100,\n    temperature=0\n)\n\n# Access the content via object attributes, not dict keys\nprint(response_json.choices[0].message.content)\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Here, we gave a small ‚Äúfew-shot‚Äù style prompt (one or two examples) so the model consistently responds in JSON format, fulfilling the Structured Output capability.","metadata":{}},{"cell_type":"markdown","source":"# **7. Results & Discussion**\n\nThis section highlights the strengths and areas for improvement of the current system. By combining embeddings, a vector store, and an LLM, we demonstrate a practical pipeline for misinformation detection.\n\n---\n\n### ‚úÖ **Observations**\n\n1. **Effective Retrieval**  \n   The system consistently retrieves fact-checks that are semantically similar to the user's claim. This shows the embedding model and FAISS index are working well for capturing conceptual similarity.\n\n2. **Grounded LLM Explanations**  \n   Because we explicitly provide supporting evidence (via top-k retrieved texts), the LLM's responses are more reliable and less prone to hallucination. The generated explanations often reflect the core sentiment and stance of the reference material.\n\n3. **Flexible Prompt Engineering**  \n   The prompt design (e.g., structured JSON, bullet points) can be customized to suit various output formats, improving downstream usability (e.g., in web apps, APIs, or analytics pipelines).\n\n---\n\n### ‚ö†Ô∏è **Limitations**\n\n1. **Coverage Gaps in the Dataset**  \n   If a user provides a **novel claim** that isn‚Äôt closely aligned with the training data, the retrieval process may return irrelevant or weak matches. This limits the model‚Äôs ability to produce an accurate explanation.\n\n2. **Dependency on Original Labels**  \n   The system assumes the original labels (true/false) in the dataset are accurate. If those fact-checks are themselves outdated, biased, or incorrect, the LLM will inherit and reinforce those flaws.\n\n3. **Lack of Temporal Awareness**  \n   The system does not account for when a claim was made or verified. In reality, facts evolve (e.g., medical advice, political events), so a static dataset can lead to stale or misleading conclusions.\n\n4. **Limited to English (Default Setup)**  \n   The model and dataset used are English-only. Multilingual support would be required to scale this to global misinformation detection.\n\n---\n\n### üí° **Suggestions for Users**\n\n- Try using **different queries** to test the system‚Äôs generalization.\n- Swap in other **LLMs** (like `gpt-4`, `Claude`, or local models via Hugging Face).\n- Adjust the **prompt** to change tone, style, format (e.g., tweet-length responses, long explanations, academic citations).\n- Plug in your **own dataset** to adapt the pipeline for domains like healthcare, education, or financial scams.\n\nThis is not a complete solution, but it‚Äôs a solid foundation that highlights how AI can assist in fact-checking workflows.\n","metadata":{}},{"cell_type":"markdown","source":"# **8. Conclusion & Future Work**\n\nIn this notebook, we demonstrated how to build a **fact-checking pipeline** using:\n\n- **Sentence Transformers** for embeddings.\n- **FAISS** for vector similarity search.\n- **OpenAI‚Äôs** language model to generate concise, grounded explanations.\n\n### Possible Next Steps\n1. **Multilingual Support**: Incorporate models and data for different languages.\n2. **Real-Time Data**: Pull live content from social media or news APIs for on-the-fly fact checks.\n3. **Improved Prompt Engineering**: Use few-shot prompts or chain-of-thought to yield more rigorous explanations.\n4. **Deploy as an App**: Build a simple web or command-line interface to let users query claims interactively.\n\nFinally, **have fun experimenting**! Change the model, the vector database, or the prompt. This is just one example pipeline‚Äîthe underlying approach can be adapted to many tasks beyond fake news detection.\n","metadata":{}},{"cell_type":"markdown","source":"# **Appendix: References & Acknowledgments**\n\n1. **ISOT Fake News Dataset**\n2. [Snopes](https://www.snopes.com/) ‚Äì Fact-check references\n3. [PolitiFact](https://www.politifact.com/)\n4. [SentenceTransformers Documentation](https://www.sbert.net/)\n5. [FAISS Documentation](https://github.com/facebookresearch/faiss)\n6. [OpenAI API Documentation](https://platform.openai.com/docs/introduction)\n\nIf you‚Äôd like to switch to other vector stores (e.g., Pinecone, Chroma) or other LLM providers (e.g., Hugging Face‚Äôs Transformers, Cohere), feel free to adapt the code to suit your requirements.\n","metadata":{}}]}