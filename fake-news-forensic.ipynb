{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":2345298,"sourceType":"datasetVersion","datasetId":1415853}],"dockerImageVersionId":30918,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# **Fake News Forensic – Detecting & Summarizing Misinformation with GenAI**\n\nThis notebook demonstrates how to detect and summarize misinformation (so-called “fake news”) by combining:\n\n1. **Embeddings** to transform text into vector representations.\n2. A **Vector Store** (in this example, using FAISS) to enable similarity search.\n3. **Retrieval-Augmented Generation (RAG)** techniques to ground outputs from a Large Language Model (LLM) in factual data.\n\nWe will walk through each step—from data loading and vector database creation to prompting a language model for fact-checking responses. Feel free to **experiment** by changing prompts, using different LLMs, or customizing the data!\n\n","metadata":{}},{"cell_type":"markdown","source":"# **1. Introduction**\n\nMisinformation spreads rapidly online, and manual fact-checking cannot always keep pace. In this project, we show how **Generative AI** methods can help:\n\n1. **Find** relevant fact-check statements from a curated dataset (using embeddings + a vector database).\n2. **Generate** short, evidence-based explanations (using a language model like OpenAI’s GPT).\n3. **Structure** responses in a user-friendly format, such as JSON or bullet-point summaries.\n\nOur example uses the **ISOT Fake News Dataset** (which labels articles as true or false) and an OpenAI model to produce short explanations referencing the retrieved evidence.\n","metadata":{}},{"cell_type":"markdown","source":"# **2. Libraries & Setup**\n\nWe first need to install and import the libraries that will power our pipeline:\n\n- **sentence-transformers**: For creating text embeddings.\n- **langchain** & **chromadb**: Common libraries for building LLM applications (though we focus on FAISS here).\n- **faiss-gpu**: A vector store for similarity search.\n- **pandas**, **numpy**: For data manipulation.\n- **torch**: Underlying framework (used by sentence-transformers).\n- **openai**: To interact with OpenAI’s GPT models.\n\nInstalling might require a restart of the environment once done. Let’s go ahead and set things up.\n","metadata":{}},{"cell_type":"code","source":"!pip install --quiet --upgrade --force-reinstall \\\n  \"protobuf<5.0.0dev\" \\\n  sentence-transformers \\\n  langchain \\\n  chromadb \\\n  faiss-gpu\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-14T22:57:47.275939Z","iopub.execute_input":"2025-04-14T22:57:47.276291Z","iopub.status.idle":"2025-04-14T22:58:20.144134Z","shell.execute_reply.started":"2025-04-14T22:57:47.276260Z","shell.execute_reply":"2025-04-14T22:58:20.143174Z"}},"outputs":[{"name":"stdout","text":"^C\n\u001b[31mERROR: Operation cancelled by user\u001b[0m\u001b[31m\n\u001b[0m","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"\n\nimport pandas as pd\nimport numpy as np\nimport torch\n\n# For embeddings (SentenceTransformers)\nfrom sentence_transformers import SentenceTransformer\n\n# For vector database, example: FAISS or Chroma\nimport faiss  # or use Chroma or another\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"We import the necessary libraries for data manipulation (**pandas**, **numpy**), generating embeddings (**SentenceTransformers**), creating a vector index (**FAISS**), and working with **PyTorch** (which powers the transformer model).\n","metadata":{}},{"cell_type":"markdown","source":"**Obtain an API Key**\n\nTo use OpenAI’s API (e.g., GPT-3.5 or GPT-4), you will need an API key from [https://platform.openai.com/](https://platform.openai.com/). Note that **paid plans** or billing information might be required depending on how many requests you make and the rate limits you exceed. If you are just testing small requests, the free trial credit might suffice, but for consistent or larger-scale usage, you’ll need a paid subscription.\n\nIn a Kaggle notebook or a local Jupyter environment, you can store the key in an environment variable or a secrets manager. Below, we demonstrate retrieving it from Kaggle’s `UserSecretsClient`.\n","metadata":{}},{"cell_type":"code","source":"# For language model calls (OpenAI, Hugging Face, or local)\n# Example with OpenAI:\n\nimport os\nimport openai\nfrom kaggle_secrets import UserSecretsClient\n\nOPENAI_API_KEY = UserSecretsClient().get_secret(\"OPENAI_API_KEY\")\nopenai.api_key = OPENAI_API_KEY\n\n\n\n# ...any additional imports...\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# **3. Data Loading & Preprocessing**\n\nIn this section, we’ll load our **ISOT Fake News Dataset**, which consists of two CSV files:\n- `Fake.csv` for misinformation articles\n- `True.csv` for real news articles\n\nWe then combine them into a single DataFrame, adding a column `'label'` indicating whether the text is **true** or **false**. You can swap in **your own** data here to create a customized fact-checking pipeline.\n","metadata":{}},{"cell_type":"code","source":"# 1) Read the CSVs from the ISOT Fake News Dataset\ndf_fake = pd.read_csv('/kaggle/input/isot-fake-news-dataset/Fake.csv')\ndf_true = pd.read_csv('/kaggle/input/isot-fake-news-dataset/True.csv')\n\n# 2) Assign labels\ndf_fake['label'] = 'false'\ndf_true['label'] = 'true'\n\n# 3) Concatenate into a single DataFrame\ndf = pd.concat([df_fake, df_true], ignore_index=True)\n\n# 4) Display 3 rows from each label to get a quick look\ndf_subset = df.groupby('label').head(3)\ndf_subset\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"You can see a snippet of the data above. Each row contains text (the article body or headline) and the `label` indicating **true** or **false**.\n","metadata":{}},{"cell_type":"markdown","source":"# **4. Building & Populating the Vector Store**\n\nIn this stage, we will:\n\n1. **Generate embeddings** for each article/claim using a Sentence Transformers model.\n2. **Store** those embeddings in a FAISS index.\n\nA vector store allows us to quickly find articles similar to any new query. This is critical for **retrieval-augmented generation** because we can feed the top-matching articles to the language model to ground its responses in factual data.\n\n---\n\n**Tip: Enable GPU for Faster Embedding & Index Building**\n\n- In the **Kaggle** Notebook, go to **Settings** on the right side of the screen and switch the Hardware Accelerator to **GPU**.  \n- If `torch.cuda.is_available()` returns `True`, you can specify `device='cuda'` when creating your `SentenceTransformer` model:\n  \n  ```python\n  import torch\n  device = 'cuda' if torch.cuda.is_available() else 'cpu'\n  embedding_model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2', device=device)\n\n","metadata":{}},{"cell_type":"code","source":"# Choose a sentence-transformers model (lightweight example)\nembedding_model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2', device='cuda')\n\n# Create a list of texts to embed (e.g., headlines or claims)\ntexts = df['text'].tolist()\nlabels = df['label'].tolist()\n\n# Generate embeddings\nembeddings = embedding_model.encode(texts, show_progress_bar=True)\n\nprint(\"Embeddings shape:\", embeddings.shape)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Here, we use the **all-MiniLM-L6-v2** model, which produces 384-dimensional vector embeddings. If you prefer a different model (e.g., `all-mpnet-base-v2` or a multilingual model), simply replace the string in `SentenceTransformer(...)`.\n","metadata":{}},{"cell_type":"markdown","source":"**4.2 Creating a FAISS Index**\n\nNext, we create a **FAISS** index to store these embeddings. FAISS supports efficient similarity search, letting us retrieve the top-k most similar items for any new query.\n\nYou could use other vector databases (e.g., **Chroma**, **Milvus**, **Pinecone**, etc.) if you prefer.\n","metadata":{}},{"cell_type":"code","source":"dimension = embeddings.shape[1]\nindex = faiss.IndexFlatL2(dimension)\nindex.add(embeddings)\n\nprint(f\"FAISS index size: {index.ntotal}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"The index is now created and populated. We can quickly query it with any embedding vector to find similar texts from our dataset.\n","metadata":{}},{"cell_type":"markdown","source":"# **5. Retrieval Augmented Generation (RAG)**\n\nWith the vector store in place, we can **retrieve** the top relevant articles for a new claim/headline. Then, we’ll feed both the user’s query and the retrieved evidence into an LLM to generate a short, fact-based explanation.\n\nThis approach helps **reduce hallucinations** by giving the model actual reference text for each claim. Let’s define some helper functions next.\n","metadata":{}},{"cell_type":"markdown","source":"**5.1 Example: Single Query with RAG**\n\nBelow, we:\n1. Create a function to **retrieve** the top-k similar entries using the FAISS index.\n2. Create a function to **generate** a short explanation by referencing those retrieved entries in the LLM prompt.\n3. Test the workflow with an example query (e.g., `\"Vaccines cause autism.\"`).\n\nYou can swap in **any query** you’d like here. Also, if you want to use GPT-4 or a different model, update the `model` parameter in the `openai.chat.completions.create(...)` call.\n","metadata":{}},{"cell_type":"code","source":"\nimport openai\n\ndef retrieve_similar_texts(query, k=3):\n    \"\"\"\n    Given a query (string), return top k similar text entries\n    from the FAISS index.\n    \"\"\"\n    query_emb = embedding_model.encode([query])\n    distances, indices = index.search(query_emb, k)\n\n    results = []\n    for idx in indices[0]:\n        item_text = df.loc[idx, 'text']\n        item_label = df.loc[idx, 'label']\n        results.append((item_text, item_label))\n    return results\n\ndef generate_explanation(query, retrievals):\n    \"\"\"\n    Use the retrieved text & label to produce a short explanation\n    via ChatCompletion in openai>=1.0.0.\n    \"\"\"\n\n    references_str = \"\\n\".join([f\"Claim: {rt[0]} -- Label: {rt[1]}\" for rt in retrievals])\n\n    messages = [\n        {\n            \"role\": \"system\",\n            \"content\": \"You are a fact-checking assistant.\"\n        },\n        {\n            \"role\": \"user\",\n            \"content\": f\"\"\"\nThe user says: '{query}'\n\nWe found these fact-check references:\n{references_str}\n\nBased on these, is the user's claim likely true or false?\nProvide a concise explanation (2-3 sentences) referencing the evidence above.\n            \"\"\"\n        }\n    ]\n\n    # For openai>=1.0.0, we use the 'chat.completions.create' endpoint\n    response = openai.chat.completions.create(\n        model=\"gpt-3.5-turbo\",  # or gpt-4, etc.\n        messages=messages,\n        max_tokens=150,\n        temperature=0\n    )\n\n    # Access the content by attribute, not by subscripting\n    return response.choices[0].message.content.strip()\n\n# Test call\nquery_test = \"Vaccines cause autism.\"\nretrieved = retrieve_similar_texts(query_test, k=3)\nllm_explanation = generate_explanation(query_test, retrieved)\n\nprint(\"----Retrieved Fact Checks----\")\nfor i, (txt, lbl) in enumerate(retrieved):\n    print(f\"{i+1}) [Label: {lbl}] {txt[:80]}...\")\n\nprint(\"\\n----LLM Explanation----\")\nprint(llm_explanation)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"The prompt to the LLM includes both the user’s claim and the top matching fact-check entries. Feel free to **tweak** the instructions to get a more structured output, longer explanation, or bullet-point summary. This is a key part of **prompt engineering**.\n","metadata":{}},{"cell_type":"markdown","source":"# **6. Demonstration of Additional Capabilities**\n\nBelow is an example of **few-shot prompting** to ensure that the LLM outputs data in a consistent format (e.g., JSON). This can be helpful if you want to parse the output programmatically later on.\n","metadata":{}},{"cell_type":"markdown","source":"**Few-Shot Prompting Example**\n\nWe might want the explanation in a more structured JSON format or a “bullet-point” style. Let’s do a few-shot approach to ensure consistent formatting.","metadata":{}},{"cell_type":"code","source":"import openai\n\ndemo_prompt = \"\"\"\nBelow are examples of how to respond to fact-check queries in JSON:\n\nExample 1:\n{\n  \"label\": \"false\",\n  \"explanation\": \"This claim is contradicted by evidence in X and Y...\"\n}\n\nNow, follow that format exactly:\n\nUser Claim: \"5G towers spread COVID-19.\"\nEvidence: \"Claim: 5G networks cause coronavirus. Label: false\"\n\n{\n   \"label\": \n   \"explanation\":\n}\n\"\"\"\n\n# In openai>=1.0.0, you call openai.chat.completions.create(...)\nresponse_json = openai.chat.completions.create(\n    model=\"gpt-3.5-turbo\", \n    messages=[\n        {\n            \"role\": \"system\",\n            \"content\": \"You are a helpful assistant that outputs JSON.\"\n        },\n        {\n            \"role\": \"user\",\n            \"content\": demo_prompt\n        }\n    ],\n    max_tokens=100,\n    temperature=0\n)\n\n# Access the content via object attributes, not dict keys\nprint(response_json.choices[0].message.content)\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Here, we gave a small “few-shot” style prompt (one or two examples) so the model consistently responds in JSON format, fulfilling the Structured Output capability.","metadata":{}},{"cell_type":"markdown","source":"# **7. Live News Detection with Gemini**\n\nOur current model only works on static data. But what if we want to fact-check **breaking news** or **recent headlines**?\n\nLet’s use:\n- **NewsAPI** to pull recent articles (e.g., about AI)\n- **Gemini** to analyze the truthfulness of these headlines\n\nThis allows us to fact-check claims in **real time**.\n\n","metadata":{}},{"cell_type":"code","source":"# ✅ Install required libraries (skip if already installed)\n!pip install -q google-generativeai newsapi-python\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 🔐 Setup API keys and libraries\nimport os\nimport google.generativeai as genai\nfrom newsapi import NewsApiClient\nfrom kaggle_secrets import UserSecretsClient\n\n# Get your Gemini and NewsAPI keys (from Kaggle Secrets)\nGOOGLE_API_KEY = UserSecretsClient().get_secret(\"GOOGLE_API_KEY\")\nNEWS_API_KEY = UserSecretsClient().get_secret(\"NEWS_API_KEY\")\n\ngenai.configure(api_key=GOOGLE_API_KEY)\nnewsapi = NewsApiClient(api_key=NEWS_API_KEY)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# List available models (must be authenticated first)\nimport google.generativeai as genai\ngenai.configure(api_key=GOOGLE_API_KEY)\n\nfor model in genai.list_models():\n    print(model.name)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 🗞️ Function to fetch live news headlines\ndef fetch_latest_news(topic=\"technology\", language=\"en\", page_size=5):\n    articles = newsapi.get_everything(q=topic, language=language, sort_by=\"publishedAt\", page_size=page_size)\n    return [article['title'] + \". \" + article.get('description', '') for article in articles['articles']]\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 🤖 Use Gemini to fact-check a headline (without grounding)\ndef gemini_fact_check(claim_text):\n    \"\"\"\n    Uses Gemini to determine whether a claim is true, false, or uncertain.\n    \"\"\"\n    model = genai.GenerativeModel('models/gemini-1.5-pro')  # Stable and widely supported\n    \n    prompt = f\"\"\"You are a helpful AI that checks if the following recent news claim is likely accurate, partially true, or false.\n\nClaim: \"{claim_text}\"\n\nWithout guessing, briefly explain your reasoning in 2–3 sentences. If you're unsure, say so clearly.\"\"\"\n    response = model.generate_content(prompt)\n    return response.text\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 🧪 Example: Fact-checking 3 live AI news headlines\nlive_headlines = fetch_latest_news(topic=\"AI\", page_size=3)\n\nfor i, headline in enumerate(live_headlines):\n    print(f\"\\n📌 Headline {i+1}: {headline}\")\n    result = gemini_fact_check(headline)\n    print(f\"🤖 Gemini Response:\\n{result}\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"> ✅ **Tip:** The more specific the topic you search (e.g., \"ChatGPT security\", \"AI legislation\"), the more focused your fact-checking pipeline becomes.\n","metadata":{}},{"cell_type":"markdown","source":"# **8. Enhancing Accuracy: Google Search Grounding**\n\nGemini is powerful, but it can still hallucinate or guess if it's not grounded in real-world context.\nTo boost accuracy, we’ll combine it with **Google Programmable Search** to pull top search snippets.\n\n---\n\n## Setup: Get Your Google Search API Key & Search Engine ID (CX)\n\nFollow these steps to enable live search:\n\n### Step 1: Create a Programmable Search Engine\n1. Go to [programmablesearchengine.google.com](https://programmablesearchengine.google.com/)\n2. Click \"Create a search engine\"\n3. Under \"Sites to search,\" enter:\n   `www.google.com`\n4. Name it anything (e.g., \"NewsVerifier\")\n5. Click Create\n\n### Step 2: Get Your Search Engine ID (CX)\n- Go to [My Search Engines](https://programmablesearchengine.google.com/cse/all)\n- Click your engine and copy the Search engine ID (your CX)\n\n### Step 3: Get Your Google API Key\n1. Go to the [Google Cloud Console](https://console.cloud.google.com/)\n2. Create or select a project\n3. Go to APIs & Services > Credentials\n4. Click \"+ Create Credentials > API key\" and copy the result\n\n### Step 4: Enable the Custom Search API\n1. In the Cloud Console, go to Library\n2. Search for \"Custom Search API\"\n3. Click it → Click \"Enable\"\n\n---\n\n### You will need two values:\n- `GOOGLE_SEARCH_API_KEY` → from Google Cloud Console\n- `GOOGLE_SEARCH_ENGINE_ID` (CX) → from the Programmable Search dashboard\n\nStore them securely using `UserSecretsClient()` in Kaggle or environment variables locally.\n","metadata":{}},{"cell_type":"code","source":"# Google Programmable Search setup\nimport requests\n\n# Get your Google Search credentials (from Kaggle Secrets)\nGOOGLE_SEARCH_API_KEY = UserSecretsClient().get_secret(\"GOOGLE_SEARCH_API_KEY\")\nGOOGLE_SEARCH_ENGINE_ID = UserSecretsClient().get_secret(\"GOOGLE_SEARCH_ENGINE_ID\")\n\n# Search Google and return top search result snippets (with title)\ndef google_search_snippets(query, num_results=5, site_filter=None):\n    \"\"\"\n    Uses Google Custom Search API to retrieve top search result snippets.\n    \n    Args:\n        query (str): The search term or claim.\n        num_results (int): Number of search results to return.\n        site_filter (str, optional): Domain to restrict search (e.g., \"snopes.com\").\n        \n    Returns:\n        List[str]: Formatted list of title + snippet strings.\n    \"\"\"\n    search_query = f\"site:{site_filter} {query}\" if site_filter else query\n    url = \"https://www.googleapis.com/customsearch/v1\"\n\n    params = {\n        \"key\": GOOGLE_SEARCH_API_KEY,\n        \"cx\": GOOGLE_SEARCH_ENGINE_ID,\n        \"q\": search_query,\n        \"num\": num_results\n    }\n\n    response = requests.get(url, params=params)\n    items = response.json().get(\"items\", [])\n\n    return [f\"{item['title']}: {item['snippet']}\" for item in items]\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**What This Cell Does**  \n- Imports the `requests` library to handle HTTP requests.  \n- Retrieves **Google Search credentials** (`GOOGLE_SEARCH_API_KEY` and `GOOGLE_SEARCH_ENGINE_ID`) via `UserSecretsClient`.\n- Defines a function `google_search_snippets(query, num_results=5, site_filter=None)`:\n  - Constructs a query URL for the Google Custom Search API.\n  - Executes the search by sending an HTTP GET request.\n  - Extracts each result’s `title` and `snippet` into a list of formatted strings.\n\n**Suggestions / Tips**  \n- If you want to limit your search to a specific domain, pass a domain as `site_filter` (e.g., `\"snopes.com\"`). We will do this next.  \n- Adjust `num_results` to retrieve more or fewer search results as needed.  \n- Be mindful of API rate limits from Google.","metadata":{}},{"cell_type":"code","source":"def grounded_fact_check_with_snopes_and_gemini(claim_text):\n    \"\"\"\n    Uses Google search on snopes.com, politifact.com, and the general web for evidence,\n    then sends structured evidence to Gemini for grounded fact-checking.\n    \"\"\"\n\n    # Step 1: Retrieve search evidence\n    snopes_evidence = google_search_snippets(claim_text, site_filter=\"snopes.com\", num_results=5)\n    politifact_evidence = google_search_snippets(claim_text, site_filter=\"politifact.com\", num_results=5)\n    general_evidence = google_search_snippets(claim_text, num_results=5)\n\n    # Step 2: Combine all search snippets\n    all_evidence = snopes_evidence + politifact_evidence + general_evidence\n\n    # Format the snippets as bullet points\n    context = \"\\n\".join([f\"- {snippet}\" for snippet in all_evidence])\n\n    # Step 3: Prompt Gemini with reasoning steps\n    prompt = f\"\"\"\nYou are a fact-checking assistant analyzing the following claim.\n\nClaim: \"{claim_text}\"\n\nBelow is real-world evidence gathered from Snopes, PolitiFact, and general search results:\n{context}\n\n---\nYour task:\n\n1. Summarize the key points from the evidence.\n2. Analyze whether the evidence supports or contradicts the claim.\n3. Give a final verdict using ONE of the following labels:\n\n    - True\n    - False\n    - Misleading\n    - Lacks Evidence\n\nThen explain your reasoning in 2–3 sentences and state your confidence level (High, Medium, or Low).\n    \"\"\"\n\n    # Step 4: Generate Gemini response\n    model = genai.GenerativeModel('models/gemini-1.5-pro')\n    response = model.generate_content(prompt)\n    \n    return response.text.strip()\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**What This Cell Does**  \n- Implements `grounded_fact_check_with_snopes_and_gemini`.\n  1. Performs targeted searches on **snopes.com**, **politifact.com**, and the general web using the function from Cell 36.\n  2. Combines the returned snippets into a single list, then formats them as bullet points.\n  3. Constructs a prompt directing Gemini to:\n     - Summarize key points from the evidence.\n     - Decide if the claim is `True`, `False`, `Misleading`, or `Lacks Evidence`.\n     - Provide a brief explanation with a stated confidence level.\n  4. Calls **Gemini** (`genai.GenerativeModel('models/gemini-1.5-pro')`) to generate the AI’s verdict.\n\n**Suggestions / Tips**  \n- You can extend or customize the prompt for a different style of output, such as bullet points or JSON.  \n- Consider adding more sources (e.g., fact-checking websites) to improve reliability.  \n- If certain sources are more trustworthy for your domain, prioritize them or filter out irrelevant domains.","metadata":{}},{"cell_type":"code","source":"topics = [\"AI\", \"Politics\", \"Healthcare\", \"Climate\", \"Finance\", \"Education\", \"Technology\", \"Environment\"]\nlive_headlines = []\n\n# Try fetching 1 headline per topic until you get 5 total\nfor topic in topics:\n    if len(live_headlines) >= 5:\n        break\n    try:\n        headlines = fetch_latest_news(topic=topic, page_size=1)\n        if headlines:\n            live_headlines.extend(headlines)\n    except Exception as e:\n        print(f\"Error fetching topic '{topic}': {e}\")\n\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**What This Cell Does**  \n- Creates a list of **topics** (`\"AI\"`, `\"Politics\"`, `\"Healthcare\"`, etc.).  \n- Fetches one headline per topic using `fetch_latest_news`, aiming to collect **5 total** headlines.  \n- Catches any exceptions that occur (for instance, API errors or empty responses).\n\n**Suggestions / Tips**  \n- Adjust the topics to match your specific interests (e.g., `\"Sports\"`, `\"Economy\"`, or `\"Entertainment\"`).  \n- Increase `page_size` if you want more headlines per topic.  \n- If you’re working on a specialized domain, replace `fetch_latest_news` with your own custom data source.","metadata":{}},{"cell_type":"code","source":"for i, headline in enumerate(live_headlines):\n    print(f\"{i+1}. {headline}\\n\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Simply **displays** each headline from the `live_headlines` list with an index. \n\nLooking at what you've generated, what do you think would be the verdict?","metadata":{}},{"cell_type":"code","source":"for i, headline in enumerate(live_headlines):\n    print(\"=\" * 100)\n    print(f\"\\nHeadline {i+1}:\\n{headline}\\n\")\n\n    verdict = grounded_fact_check_with_snopes_and_gemini(headline)\n    print(f\"GEMINI Verdict:\\n{verdict}\")\n    print(\"=\" * 100)\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Prints the AI’s **verdict** and reasoning, effectively completing a “live” fact-check based on up-to-date search evidence.\n\n**Suggestions / Tips**  \n- Enhance trust by storing the raw evidence or displaying it alongside the verdict.  \n- If Gemini’s verdict is too general, refine the prompt instructions in `grounded_fact_check_with_snopes_and_gemini`.  \n- For repeated queries or large batches of headlines, be aware of API usage and potential costs/limits.","metadata":{}},{"cell_type":"markdown","source":"# **9. Results & Observations**\n\nThis section highlights the **strengths** and **limitations** of the pipeline.\n\n### What Works Well\n- **Accurate Retrieval**: The vector store consistently returns relevant fact-checks.\n- **LLM Grounding**: RAG plus grounding boosts factual accuracy.\n- **Prompt Flexibility**: Can adapt format for different use cases (JSON, bullet points, etc.).\n\n### Limitations\n- **Static Dataset**: The base system doesn’t update with new claims unless extended.\n- **LLM Bias**: Explanations rely on prompt quality and search evidence.\n- **Language Restriction**: Default setup works only for English.\n\n### Suggestions\n- Swap in your own dataset for domain-specific tasks.\n- Use different LLMs (Claude, GPT-4, LLaMA).\n- Deploy as a web app or Slack bot.\n\n","metadata":{}},{"cell_type":"markdown","source":"# **10. Conclusion & Future Work**\n\nIn this notebook, we built a pipeline for detecting and summarizing misinformation using:\n\n- **Sentence Transformers** for embeddings.\n- **FAISS** for fast similarity search.\n- **Gemini** for generating reasoned explanations.\n- **NewsAPI & Google Search** to enable live and grounded fact-checking.\n\n### Possible Next Steps\n- Add **multilingual support**.\n- Improve UI/UX for non-technical users.\n- Deploy via web dashboard or chatbot.\n- Add **search filters** (for example, news within 24 hours).\n\nFeel free to customize and expand this project!\n","metadata":{}},{"cell_type":"markdown","source":"# **11. References & Acknowledgments**\n\n1. **ISOT Fake News Dataset**\n2. [Snopes](https://www.snopes.com/)\n3. [PolitiFact](https://www.politifact.com/)\n4. [SentenceTransformers Documentation](https://www.sbert.net/)\n5. [FAISS GitHub](https://github.com/facebookresearch/faiss)\n6. [OpenAI API Docs](https://platform.openai.com/docs)\n7. [NewsAPI](https://newsapi.org/)\n8. [Google Programmable Search](https://programmablesearchengine.google.com/)\n\nThanks for exploring this with me! \n","metadata":{}}]}